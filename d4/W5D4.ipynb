{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "W5D4.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitalminimal/w5/blob/main/d4/W5D4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC2cc2ugYfPM"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "YlBf8kJEYfPQ",
        "outputId": "5c6d643d-421f-4374-874d-21716885f5ae"
      },
      "source": [
        "Image(\"img/1.png\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "img/1.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8vIv1Z1uYfPR",
        "outputId": "e233c69f-2ffd-4651-ba23-159cba843e08"
      },
      "source": [
        "Image(\"img/2.png\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "img/2.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVvILyXGYfPR",
        "outputId": "84f35467-4304-47f2-a44b-7e3ad638c085"
      },
      "source": [
        "# And import the libraries\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import HTML\n",
        "\n",
        "%pylab inline\n",
        "!pip install git+git://github.com/mgelbart/plot-classifier.git #plot_classifier install\n",
        "from plot_classifier import plot_classifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfTransformer,\n",
        "    TfidfVectorizer,\n",
        ")\n",
        "\n",
        "# train test split and cross validation\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    cross_val_score,\n",
        "    cross_validate,\n",
        "    train_test_split,\n",
        ")\n",
        "\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Collecting git+git://github.com/mgelbart/plot-classifier.git\n",
            "  Cloning git://github.com/mgelbart/plot-classifier.git to /tmp/pip-req-build-o9ba87mx\n",
            "  Running command git clone -q git://github.com/mgelbart/plot-classifier.git /tmp/pip-req-build-o9ba87mx\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from plot-classifier==0.1) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=2.1 in /usr/local/lib/python3.7/dist-packages (from plot-classifier==0.1) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->plot-classifier==0.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->plot-classifier==0.1) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->plot-classifier==0.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->plot-classifier==0.1) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.1->plot-classifier==0.1) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJGPce-aYfPS"
      },
      "source": [
        "## Review: Conditional probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVeE_NKvYfPT"
      },
      "source": [
        "- Conditional probabilities are a way of using information we have about random variables.\n",
        "- For example, for a fair 6-sided dice, what if we already know the roll is odd, because someone told us. What are the 6 _conditional_ probabilities of each outcome?\n",
        "  - They are: {3: 1/3, 5: 1/3, 1: 1/3, 2: 0/3, 4: 0/3, 6: 0/3}\n",
        "- We write conditional probabilities with a vertical bar, `|`. The information we're conditioning on goes after the bar.\n",
        "  - E.g., $P(X=i \\mid \\text{X is odd})$ is a **conditional probability**\n",
        "  - The set of these values form the **conditional distribution**\n",
        "    - Conditional distributions are still probability distributions - they must sum up to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKW48iEkYfPU"
      },
      "source": [
        "- So, what's the pattern here?\n",
        "  - The conditioning _eliminates some possible outcomes_.\n",
        "  - For the remaining outcomes, we _renormalized_ the distribution.\n",
        "  - That is, we took the proportion of the allowed outcomes that satisfy the event description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77lo31RYfPU"
      },
      "source": [
        "### Conditional probabilities - formalizing things\n",
        "\n",
        "The key equation with conditional probabilities is\n",
        "\n",
        "$$P(A\\mid B)=\\frac{P(A \\cap B)}{P(B)}$$\n",
        "\n",
        "The \"renormalizing\" trick is a consequence of this. \n",
        "\n",
        "Consider, what's the probability of rolling a 6 given that the roll is not 1?\n",
        "\n",
        "- Let $A$ be the roll is a 6\n",
        "- Let $B$ be the roll is a not a 1\n",
        "\n",
        "$$P(A\\mid B)=\\frac{P(A \\cap B)}{P(B)}=\\frac{P(A)}{P(B)}=\\frac{1/6}{5/6}=\\frac{1}{5}$$\n",
        "\n",
        "In this case, we had the simplification that $P(A\\cap B)=P(A)$. WHY? This is often not the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB8X-J9kYfPU"
      },
      "source": [
        "### Bayes' Theorem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "KZEuJC6WYfPU",
        "outputId": "e7844469-9c2b-424e-86c2-e29bc8615fa5"
      },
      "source": [
        "Image(\"img/bayesthm.jpeg\",width = 600, height = 300)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "img/bayesthm.jpeg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 600,
              "height": 300
            }
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "AIr8gSwrYfPV",
        "outputId": "0626b311-73fa-4d59-c01a-ac12ab4fe847"
      },
      "source": [
        "Image(\"img/bayesexample.PNG\",width = 600, height = 300)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "img/bayesexample.PNG",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 600,
              "height": 300
            }
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPVT9YrwYfPV"
      },
      "source": [
        "**Another Example:** A heritable disease occurs randomly in 10% of the population. If someone has the disease, it is passed on to their children with probability 50%. A mother has 1 healthy child. Given this, what's the conditional probability that the mother has the disease? \n",
        "\n",
        "- Is the answer 10%? Less? More? How do we quantify it?\n",
        "  - Let $M$ be the event that the mother has the disease.\n",
        "  - Let $C$ be the event that the child has the disease.\n",
        "  - We want $P(M\\mid \\textrm{not } C)$. We have $P(M)=0.1$ and $P(\\textrm{not }C\\mid M)=0.5$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBvD0i2YYfPV"
      },
      "source": [
        "Solution:\n",
        "\n",
        "$$P(M \\mid \\textrm{not } C) = \\frac{P(\\textrm{not } C \\mid M)P(M)}{P(\\textrm{not } C)}$$\n",
        "\n",
        "So we still need $P(\\textrm{not } C)$. This could happen in 2 ways (\"law of total probability\")\n",
        "\n",
        "$$P(\\textrm{not } C)=P(\\textrm{not } C \\mid M)P(M) + P(\\textrm{not } C \\mid \\textrm{not } M)P(\\textrm{not } M)$$\n",
        "\n",
        "We know $P(\\textrm{not } M)=1-P(M)=0.9$.   \n",
        "We assume $P( C \\mid \\textrm{not } M)=0.1$ because the child can randomly get the disease like anyone else,   \n",
        "so then $P(\\textrm{not } C \\mid \\textrm{not } M)=1-P( C \\mid \\textrm{not } M)=0.9$. \n",
        "\n",
        "Finally, then, we're left with:\n",
        "\n",
        "$$P(M \\mid \\textrm{not } C) = \\frac{0.5 \\times 0.1}{0.5\\times 0.1 + 0.9 \\times 0.9} = 0.058$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUU_9M9BYfPW"
      },
      "source": [
        "- We can get what we need using **Bayes' Theorem**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1sHQzIsYfPW"
      },
      "source": [
        "If curious, you should also review:\n",
        "- Law of Total Probability  $P(X=x)=\\sum_y P(X=x\\mid Y=y)P(Y=y)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnYsh0MyYfPW"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "- For years, best spam filtering methods used naive Bayes.\n",
        "- Our first probabilistic classifier where we **think of learning as a problem of statistical inference**.\n",
        "\n",
        "- Classification technique based on Bayes’ Theorem **with an assumption of independence among predictors** - hence the Naive. \n",
        "    - The presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
        "    - This is like saying: If you are hungry the probability of the symptoms(growling stomach, mouth watering, weakness,...) manifesting are independant \n",
        "\n",
        "E.g. You receive a spam mail that contains the words \"Money\", \"URGENT!\", \"Prize!\". Even if these features depend on each other or others, all of these properties independently contribute to the probability that this email is SPAM.\n",
        "\n",
        "- Naive Bayes is easy to build and useful for very large data sets. \n",
        "\n",
        "- Naive Bayes outperforms even highly sophisticated classification methods and works well with text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prkLHx_vYfPX"
      },
      "source": [
        "## Naive Bayes Classifier\n",
        "\n",
        "\n",
        "Before understanding the theory, let's try `scikit-learn`'s implementation of Naive Bayes on Kaggle's [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS_DBXyQYfPX"
      },
      "source": [
        "#### We will use `CountVectorizer` to get bag-of-words (BOW) representation - We will come back to this later in more detail!\n",
        "\n",
        "- So we used `CountVectorizer` to convert text data into feature vectors where\n",
        "    - each feature is a unique word in the text  \n",
        "    - each feature value represents the frequency or presence/absence of the word in the given message         \n",
        "    \n",
        "<img src='./img/bag-of-words.png' width=\"600\">\n",
        "\n",
        "[Source](https://web.stanford.edu/~jurafsky/slp3/4.pdf)       "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAZbs733YfPX"
      },
      "source": [
        "sms_df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
        "sms_df = sms_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
        "sms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or1rkNMSYfPX"
      },
      "source": [
        "train_df, test_df = train_test_split(sms_df, test_size=0.2, random_state=123)\n",
        "X_train, y_train = train_df[\"sms\"], train_df[\"target\"]\n",
        "X_test, y_test = test_df[\"sms\"], test_df[\"target\"]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "4sQiricTYfPY",
        "outputId": "ad7bd46b-ad98-47b0-9404-8371a1b1332d"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>sms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>ham</td>\n",
              "      <td>It took Mr owl 3 licks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4003</th>\n",
              "      <td>ham</td>\n",
              "      <td>Well there's a pattern emerging of my friends telling me to drive up and come smoke with them and then telling me that I'm a weed fiend/make them smoke too much/impede their doing other things so ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>ham</td>\n",
              "      <td>Yes i thought so. Thanks.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2327</th>\n",
              "      <td>spam</td>\n",
              "      <td>URGENT! Your mobile number *************** WON a å£2000 Bonus Caller prize on 10/06/03! This is the 2nd attempt to reach you! Call 09066368753 ASAP! Box 97N7QP, 150ppm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103</th>\n",
              "      <td>ham</td>\n",
              "      <td>Aiyah sorry lor... I watch tv watch until i forgot 2 check my phone.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     target                                                                                                                                                                                                      sms\n",
              "385     ham                                                                                                                                                                                   It took Mr owl 3 licks\n",
              "4003    ham  Well there's a pattern emerging of my friends telling me to drive up and come smoke with them and then telling me that I'm a weed fiend/make them smoke too much/impede their doing other things so ...\n",
              "1283    ham                                                                                                                                                                                Yes i thought so. Thanks.\n",
              "2327   spam                                  URGENT! Your mobile number *************** WON a å£2000 Bonus Caller prize on 10/06/03! This is the 2nd attempt to reach you! Call 09066368753 ASAP! Box 97N7QP, 150ppm\n",
              "1103    ham                                                                                                                                     Aiyah sorry lor... I watch tv watch until i forgot 2 check my phone."
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGnGCaK2YfPY",
        "outputId": "4a2ad4c9-6dcd-495a-d890-366a08ded5c3"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "pipe_nb = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "pipe_nb.fit(X_train, y_train)\n",
        "print(\"Training Acc.: \", pipe_nb.score(X_train,y_train))\n",
        "print(\"Valid Acc.: \", pipe_nb.score(X_test,y_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Acc.:  0.9932690150325331\n",
            "Valid Acc.:  0.9865470852017937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAX_pibBYfPY"
      },
      "source": [
        "### Naive Bayes `predict`\n",
        "\n",
        "- Given a new message, we want to predict whether it's spam or non spam (ham).\n",
        "- Example: Predict whether the following message is spam or non spam (ham). \n",
        "> \"URGENT! Free!!\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz8MmR42YfPY",
        "outputId": "1bb93c2c-9182-46ee-816d-d85646fa369e"
      },
      "source": [
        "deploy_test = [\"URGENT! Free!!\", \"I like learning about stats!\"]\n",
        "pipe_nb.predict(deploy_test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['spam', 'ham'], dtype='<U4')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMov9PScYfPZ"
      },
      "source": [
        "### Probabilistic classifiers: `predict` by hand \n",
        "\n",
        "- What's it's doing under the hood? \n",
        "- Let's look at an example with a toy dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDpVITYWYfPZ"
      },
      "source": [
        "X = [\n",
        "    \"URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!\",\n",
        "    \"Lol you are always so convincing.\",\n",
        "    \"Block 2 has interesting courses.\",\n",
        "    \"URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!\",\n",
        "    \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!\",\n",
        "    \"Block 2 has been interesting so far.\",\n",
        "]\n",
        "y = [\"spam\", \"non spam\", \"non spam\", \"spam\", \"spam\", \"non spam\"]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wVuMEB4YfPZ"
      },
      "source": [
        "pipe_nb_toy = make_pipeline(CountVectorizer(max_features = 4, stop_words='english'), MultinomialNB())\n",
        "pipe_nb_toy.fit(X, y);"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTwVuiXPYfPa"
      },
      "source": [
        "data = pipe_nb_toy['countvectorizer'].transform(X)\n",
        "train_bow_df = pd.DataFrame(data.toarray(), columns=pipe_nb_toy['countvectorizer'].get_feature_names(), index=X)\n",
        "train_bow_df['target'] = y"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "VpSR2vaLYfPa",
        "outputId": "8a73faca-0b6c-47a4-9ac7-3726beba4658"
      },
      "source": [
        "train_bow_df"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block</th>\n",
              "      <th>free</th>\n",
              "      <th>prize</th>\n",
              "      <th>urgent</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lol you are always so convincing.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has interesting courses.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has been interesting so far.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                              block  ...    target\n",
              "URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!                      0  ...      spam\n",
              "Lol you are always so convincing.                                                                                 0  ...  non spam\n",
              "Block 2 has interesting courses.                                                                                  1  ...  non spam\n",
              "URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!                                       0  ...      spam\n",
              "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!      0  ...      spam\n",
              "Block 2 has been interesting so far.                                                                              1  ...  non spam\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WygzqkKlYfPa"
      },
      "source": [
        "Suppose we are given text messages in `deploy_test` and we want to find the targets for these examples, how do we do it using naive Bayes?\n",
        "\n",
        "First, let's get numeric representation of our text messages. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lHIK3l2YfPa"
      },
      "source": [
        "deploy_test = [\"URGENT! Free!!\", \"I like Week 5 block better.\"]\n",
        "data = pipe_nb_toy['countvectorizer'].transform(deploy_test).toarray()\n",
        "bow_df = pd.DataFrame(data, columns=pipe_nb_toy['countvectorizer'].get_feature_names(), index=deploy_test)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "zViI0vhXYfPa",
        "outputId": "97c6577f-29cc-4677-e151-46d71a89411b"
      },
      "source": [
        "bow_df"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block</th>\n",
              "      <th>free</th>\n",
              "      <th>prize</th>\n",
              "      <th>urgent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>URGENT! Free!!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I like Week 5 block better.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             block  free  prize  urgent\n",
              "URGENT! Free!!                   0     1      0       1\n",
              "I like Week 5 block better.      1     0      0       0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2emxd12YfPa"
      },
      "source": [
        "### Naive Bayes prediction idea\n",
        "\n",
        "Suppose we want to predict whether the following message is \"spam\" or \"non spam\".\n",
        "> \"URGENT! Free!!\"\n",
        "\n",
        "Representation of the message: `[0, 1, 0, 1]`\n",
        "\n",
        "To predict the correct class, naive Bayes calculates the following probability scores using Bayes Theorem. \n",
        "\n",
        "- $P(\\text{spam} \\mid \\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent=1})$ \n",
        "- $P(\\text{non spam} \\mid  \\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent=1})$\n",
        "- **Picks the label with higher probability scores**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frlZIP3MYfPb"
      },
      "source": [
        "### Applying Bayes' theorem \n",
        "\n",
        "Uses Bayes' theorem to calculate probabilities:\n",
        "\n",
        "$$P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)}$$\n",
        "\n",
        "$$P(\\text{spam} \\mid \\text{message})= \\frac{P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1 \\mid \\text{spam}) \\times P(\\text{spam})}{P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent=1})}$$\n",
        "\n",
        "$$P(\\text{non spam} \\mid \\text{message}) = \\frac{P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1 \\mid \\text{non spam}) \\times P( \\text{non spam})}{P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent=1})}$$\n",
        "\n",
        "- $P(\\text{message})$: marginal probability that a message has the given set of words \n",
        "    - Hard to calculate but can be ignored in our scenario as it occurs in the denominator for both $P(\\text{spam} \\mid \\text{message})$ and $P(\\text{non spam} \\mid \\text{message})$.\n",
        "    - So we ignore the denominator in both cases. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjSjV_W8YfPb"
      },
      "source": [
        "### Let's focus on $P(\\text{spam} \\mid \\text{message})$\n",
        "\n",
        "- After ignoring the denominator: \n",
        "$$P(\\text{spam} \\mid \\text{message}) \\propto P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1 \\mid \\text{spam}) \\times P(\\text{spam})$$\n",
        "\n",
        "- To calculate $P(\\text{spam} \\mid \\text{message})$, we need:  \n",
        "    - $P(\\text{spam})$: marginal probability that a message is spam\n",
        "    - $P(\\text{message}\\mid\\text{spam})$: conditional probability that message has words $w_1, w_2, \\dots, w_d$, given that it is spam.\n",
        "        - Hard to calculate because it would require huge numbers of parameters and impossibly large training sets. But we need it. \n",
        "        - with $d$ binary features, how many possible \"text messages\" are there?\n",
        "        - we cannot possibly have access to all the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb0KmPttYfPc"
      },
      "source": [
        "### Naive Bayes' approximation to calculate $P(\\text{message}|\\text{spam})$\n",
        "\n",
        "- A common assmption is **naive Bayes** assumption, which states that **features are independent, conditioned on the target**. \n",
        "    - Example: In our spam classification example, **once you know that a message is spam**, the probability that the word \"urgent\" appears is independent of whether \"free\" also appeared. Think back to our analogy of getting hungry and the symptoms associated with that...\n",
        "    \n",
        "- We can write this mathematically as \n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{split}\n",
        "& P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1 \\mid \\text{spam}) \\\\\n",
        "&\\approx P(\\text{block} = 0 \\mid \\text{spam}) \\times P(\\text{free} = 1 \\mid \\text{spam}) \\times P(\\text{prize} = 0 \\mid \\text{spam}) \\times P(\\text{urgent} = 1 \\mid \\text{spam})\n",
        "\\end{split}\n",
        "\\end{equation}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItJJ9P19YfPc"
      },
      "source": [
        "### Naive Bayes' approximation\n",
        "\n",
        "- In general, \n",
        "$$P(\\text{message} \\mid \\text{spam}) = P(w_1, w_2, . . . , w_d \\mid \\text{spam}) \\approx \\prod_{i=1}^{d}P(w_i \\mid \\text{spam})$$\n",
        "\n",
        "$$P(\\text{message} \\mid \\text{non spam}) = P(w_1, w_2, . . . , w_d \\mid \\text{non spam}) \\approx \\prod_{i=1}^{d}P(w_i \\mid \\text{non spam})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRxKdaGVYfPd"
      },
      "source": [
        "### Going back to estimating $P(\\text{spam} \\mid \\text{message})$\n",
        "\n",
        "With naive Bayes' assumption, to calculate $P(\\text{spam} \\mid \\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1)$, we need the following:  \n",
        "1. Prior probability: $P(\\text{spam})$ \n",
        "2. Conditional probabilities: \n",
        "    1. $P(\\text{block} = 0 \\mid \\text{spam})$\n",
        "    2. $P(\\text{free} = 1 \\mid \\text{spam})$\n",
        "    3. $P(\\text{prize} = 0 \\mid \\text{spam})$\n",
        "    4. $P(\\text{urgent} = 1 \\mid \\text{spam})$\n",
        "\n",
        "We use our training data to calculate these probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "Gj0cXGZxYfPd",
        "outputId": "0e56f6e9-1988-4e5d-b502-bd9d638e1873"
      },
      "source": [
        "train_bow_df"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block</th>\n",
              "      <th>free</th>\n",
              "      <th>prize</th>\n",
              "      <th>urgent</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lol you are always so convincing.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has interesting courses.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has been interesting so far.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                              block  ...    target\n",
              "URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!                      0  ...      spam\n",
              "Lol you are always so convincing.                                                                                 0  ...  non spam\n",
              "Block 2 has interesting courses.                                                                                  1  ...  non spam\n",
              "URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!                                       0  ...      spam\n",
              "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!      0  ...      spam\n",
              "Block 2 has been interesting so far.                                                                              1  ...  non spam\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEZ9qhONYfPd"
      },
      "source": [
        "- Prior probability\n",
        "    - $P(\\text{spam}) = 3/6$\n",
        "    \n",
        "- Conditional probabilities\n",
        "    - What is $P(\\text{block} = 0 \\mid \\text{spam})$? \n",
        "        - Given target is spam, how often \"block\" = 0? $3/3$\n",
        "    - $P(\\text{free} = 1 \\mid \\text{spam}) = 2/3$ \n",
        "    - $P(\\text{prize} = 0 \\mid \\text{spam}) = 1/3$\n",
        "    - $P(\\text{urgent} = 1 \\mid \\text{spam}) = 2/3$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5jxW49hYfPe"
      },
      "source": [
        "### Estimating $P(\\text{spam} \\mid \\text{message})$\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{split}\n",
        "P(\\text{spam} \\mid \\text{message}) &\\propto P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1 \\mid \\text{spam}) \\times P(\\text{spam})\\\\\n",
        "&\\propto P(\\text{block} = 0 \\mid \\text{spam}) \\times P(\\text{free} = 1 \\mid \\text{spam}) \\\\\n",
        "& \\times P(\\text{prize} = 0 \\mid \\text{spam}) \\times P(\\text{urgent} = 1 \\mid \\text{spam}) \\times P(\\text{spam})\\\\\n",
        "&\\propto 3/3 \\times 2/3 \\times 1/3 \\times 2/3 \\times 3/6\\\\\n",
        "\\end{split}\n",
        "\\end{equation}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJqxcrpnYfPe",
        "outputId": "9900b43f-14ea-43f6-fa06-6f4b18e433a8"
      },
      "source": [
        "spam_prior = 3/6\n",
        "block0_spam = 3/3\n",
        "free1_spam = 2/3\n",
        "prize0_spam = 1/3\n",
        "urgent1_spam = 2/3\n",
        "print((spam_prior * block0_spam * free1_spam * prize0_spam * urgent1_spam))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07407407407407407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ARXVJEPYfPe"
      },
      "source": [
        "### Let's estimate $P(\\text{non spam} \\mid \\text{message})$\n",
        "\n",
        "With naive Bayes' assumption, to calculate $P(\\text{non spam} \\mid \\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1)$, we need the following:  \n",
        "1. Prior probability: $P(\\text{non spam})$ \n",
        "2. Conditional probabilities: \n",
        "    1. $P(\\text{block} = 0 \\mid \\text{non spam})$\n",
        "    2. $P(\\text{free} = 1 \\mid \\text{non spam})$\n",
        "    3. $P(\\text{prize} = 0 \\mid \\text{non spam})$\n",
        "    4. $P(\\text{urgent} = 1 \\mid \\text{non spam})$\n",
        "\n",
        "Again we use the data to calculate these probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "v8QWmoU8YfPe",
        "outputId": "b0580fec-13b4-4b65-ce1a-9715460b0e9d"
      },
      "source": [
        "train_bow_df"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block</th>\n",
              "      <th>free</th>\n",
              "      <th>prize</th>\n",
              "      <th>urgent</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lol you are always so convincing.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has interesting courses.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has been interesting so far.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                              block  ...    target\n",
              "URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!                      0  ...      spam\n",
              "Lol you are always so convincing.                                                                                 0  ...  non spam\n",
              "Block 2 has interesting courses.                                                                                  1  ...  non spam\n",
              "URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!                                       0  ...      spam\n",
              "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!      0  ...      spam\n",
              "Block 2 has been interesting so far.                                                                              1  ...  non spam\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Zo7RE4YfPf"
      },
      "source": [
        "- Prior probability \n",
        "    - $P(\\text{non spam}) = 3/6$\n",
        "\n",
        "- Conditional probabilities \n",
        "    - What is $P(\\text{block} = 0 \\mid \\text{non spam})$? \n",
        "        - Given target is non spam, how often \"block\" = 0? $1/3$\n",
        "    - $P(\\text{free} = 1 \\mid \\text{non spam}) = 0/3$ \n",
        "    - $P(\\text{prize} = 0 \\mid \\text{non spam}) = 3/3$\n",
        "    - $P(\\text{urgent} = 1 \\mid \\text{non spam}) = 0/3$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ynBJeg2YfPf"
      },
      "source": [
        "### Estimating $P(\\text{non spam} \\mid \\text{message})$\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{split}\n",
        "P(\\text{non spam} \\mid \\text{message}) &\\propto P(\\text{block} = 0, \\text{free} = 1, \\text{prize} = 0, \\text{urgent} = 1 \\mid \\text{non spam}) \\times P(\\text{non spam})\\\\\n",
        "&\\propto P(\\text{block} = 0 \\mid \\text{non spam}) \\times P(\\text{free} = 1 \\mid \\text{non spam}) \\\\\n",
        "& \\times P(\\text{prize} = 0 \\mid \\text{non spam}) \\times P(\\text{urgent} = 1 \\mid \\text{non spam}) \\times P(\\text{non spam})\\\\\n",
        "&\\propto 1/3 \\times 0 \\times 3/3 \\times 0 \\times 1/3\\\\\n",
        "\\end{split}\n",
        "\\end{equation}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKGFuIESYfPg",
        "outputId": "c17a166d-0237-4a50-a4ca-7b370e53ef58"
      },
      "source": [
        "non_spam_prior = 3/6\n",
        "block0_non_spam = 0/3\n",
        "free1_non_spam = 1/3\n",
        "prize0_non_spam = 1/3\n",
        "urgent1_non_spam = 2/3\n",
        "print(non_spam_prior * block0_non_spam * free1_non_spam * prize0_non_spam * urgent1_non_spam)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-G6hnZYfPg"
      },
      "source": [
        "### Naive Bayes prediction\n",
        "\n",
        "Since $(\\text{spam} \\mid \\text{message})$ (0.074) is proportional to a larger number compared to $(\\text{non spam} \\mid \\text{message})$ (0), we predict $spam$! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUkH42xiYfPg"
      },
      "source": [
        "## 2. `predict_proba`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FCowCF4YfPg"
      },
      "source": [
        "### What is our toy pipeline's prediction? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ycC103YfPg",
        "outputId": "89a3311f-646c-45df-d3af-c94896bf2ffc"
      },
      "source": [
        "deploy_test = [\"URGENT! Free!!\"]\n",
        "pipe_nb_toy.predict(deploy_test)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['spam'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSHQnCNOYfPg"
      },
      "source": [
        "### Naive Bayes classifier `predict_proba`\n",
        "- So far we have been looking into binary predictions but often a more granular information is useful. \n",
        "- Naive Bayes classifier gives you probability estimates for each class and we can get this information using `predict_proba` method of the classifier.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErYhMQjiYfPg",
        "outputId": "7e43d761-e432-4764-e377-13964e64a279"
      },
      "source": [
        "pipe_nb_toy.predict_proba(deploy_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23584906, 0.76415094]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABDgdUMCYfPg",
        "outputId": "fccb5bcc-8c78-4d45-cc65-062729108ab9"
      },
      "source": [
        "pipe_nb_toy.classes_"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['non spam', 'spam'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKUW36ioYfPg"
      },
      "source": [
        "Above: The classifier is \"76% confident\" that the class is spam! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzbYrfzzYfPh"
      },
      "source": [
        "### Predicting probabilities\n",
        "\n",
        "- We have a new and useful method, `predict_proba`.\n",
        "- `predict` returns the class with the highest probability.\n",
        "- `predict_proba` gives us the actual probability scores. \n",
        "- Looking at the probabilities can help us understand the model.\n",
        "- We can find the spam messages where our classifier is most confident and least confident. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HepOCdHAYfPh"
      },
      "source": [
        "### Why don't the calculations match?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZLrJukcYfPh"
      },
      "source": [
        "- Recall that when we worked through a toy example by hand, we estimated\n",
        "    - $P(\\text{non spam} \\mid \\text{message}) \\propto 0$\n",
        "    - $P(\\text{spam} \\mid \\text{message}) \\propto 0.074$\n",
        "- Why don't `predict_proba` scores match with the probability scores we calculated before? \n",
        "- The scores we computed are not normalized. Remember that we ignored the denominator.\n",
        "- These ones are normalized so that they sum to 1.\n",
        "- The model is using something called \"smoothing\" to avoid the problem of zero probabilities. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chT949RxYfPh"
      },
      "source": [
        "## 3. Laplace smoothing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "b63J20p9YfPh",
        "outputId": "e37beebf-b914-4bd2-e366-249ef673f9b6"
      },
      "source": [
        "train_bow_df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block</th>\n",
              "      <th>free</th>\n",
              "      <th>prize</th>\n",
              "      <th>urgent</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lol you are always so convincing.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has interesting courses.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has been interesting so far.</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                              block  ...    target\n",
              "URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!                      0  ...      spam\n",
              "Lol you are always so convincing.                                                                                 0  ...  non spam\n",
              "Block 2 has interesting courses.                                                                                  1  ...  non spam\n",
              "URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!                                       0  ...      spam\n",
              "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!      0  ...      spam\n",
              "Block 2 has been interesting so far.                                                                              1  ...  non spam\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSlPx-bYfPh"
      },
      "source": [
        "- Remember when we calculated $P(\\text{non spam} \\mid \\text{message})$, some of our conditional probabilities were zero. \n",
        "    - $P(\\text{free} = 1 \\mid \\text{non spam}) = 0/3$ \n",
        "    - $P(\\text{urgent} = 1 \\mid \\text{non spam}) = 0/3$\n",
        "\n",
        "- Naive Bayes naively multiplies all the feature likelihoods together, and if any of the terms is zero, it's going to void all other evidence and the probability of the class is going to be zero. \n",
        "- This is problematic! \n",
        "- **We have limited data and if we do not see a feature occurring with a class, it doesn't mean it would never occur with that class**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2uX2yd4YfPh"
      },
      "source": [
        "### A simple solution: Laplace smoothing\n",
        "\n",
        "- The simplest way to avoid zero probabilities is to add one to all the counts.\n",
        "- All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. \n",
        "- In `scikit-learn` we control it using hyperparameter `alpha` (by default `alpha=1.0`). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jo1g5snYfPh"
      },
      "source": [
        "So our previous bag of words representation becomes like this: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "KAI_g3ZpYfPh",
        "outputId": "cccc4212-8d63-4fbb-9525-5ebaa3a6cd6f"
      },
      "source": [
        "data = pipe_nb_toy['countvectorizer'].transform(X)\n",
        "train_bow_df = pd.DataFrame(data.toarray() + 1, columns=pipe_nb_toy['countvectorizer'].get_feature_names(), index=X)\n",
        "train_bow_df['target'] = y\n",
        "train_bow_df"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block</th>\n",
              "      <th>free</th>\n",
              "      <th>prize</th>\n",
              "      <th>urgent</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lol you are always so convincing.</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has interesting courses.</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Block 2 has been interesting so far.</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>non spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                              block  ...    target\n",
              "URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!                      1  ...      spam\n",
              "Lol you are always so convincing.                                                                                 1  ...  non spam\n",
              "Block 2 has interesting courses.                                                                                  2  ...  non spam\n",
              "URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!                                       1  ...      spam\n",
              "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!      1  ...      spam\n",
              "Block 2 has been interesting so far.                                                                              2  ...  non spam\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L84nm0sWYfPh"
      },
      "source": [
        "### Adjusting the counts \n",
        "\n",
        "Note that the following calculations would change now with updated counts now: \n",
        "\n",
        "$$P(\\text{word} \\mid \\text{spam}) = \\frac{Count(\\text{word}, \\text{spam}) + 1}{\\sum_{w \\in vocabulary} Count(w, \\text{spam}) + |vocabulary|}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiiuE5oJYfPh"
      },
      "source": [
        "### `alpha` hyperparameter and the fundamental tradeoff \n",
        "\n",
        "- High alpha $\\rightarrow$ underfitting\n",
        "    - means we are adding large counts to everything and so we are diluting the information from the data\n",
        "- Low alpha $\\rightarrow$ overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prhi4rcfYfPi"
      },
      "source": [
        "###  Gaussian Naive Bayes\n",
        "\n",
        "- If a dataset has continuous-valued features.\n",
        "- But so far, we've only seen how to use Naive Bayes for discrete features.\n",
        "- We can either discretize our continuous features into discrete bins (with counts), or...\n",
        "- Use _Gaussian_ naive Bayes (read more [here](https://machinelearningmastery.com/naive-bayes-for-machine-learning/) and [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes))\n",
        "- Now:\n",
        "    - Assume each feature is normally distributed \n",
        "    - Calculate the mean ($\\mu_k$) and standard deviation ($\\sigma_k$) for each feature for each class\n",
        "    - Use the following equation to calculate the conditional probability of observing feature value $v$ (occurence of a word in our previous example) in class $C_k$ (Spam/ham in our previous example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpkxYa6QYfPi"
      },
      "source": [
        "<img src='./img/gaus_nb.png' width=\"250\">\n",
        "\n",
        "\n",
        "Source: [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzrdQ1mPYfPi"
      },
      "source": [
        "- Gaussian naive Bayes assumes features are normally distributed\n",
        "    - Are our features normal?\n",
        "    - Not really but in practice we transform our data to try and make it more normal\n",
        "    - Scikit-learn provides the `PowerTransformer()` for this process\n",
        "    - From the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer): \"*...Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like.*\"    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj5wx9hxYfPi"
      },
      "source": [
        "### Pros of naive Bayes\n",
        "\n",
        "- Surprising accuracy \n",
        "- A fast and robust way to learn the corresponding parameters\n",
        "- Scales great; learning a naive Bayes classifier is just a matter of counting how many times each attribute co-occurs with each class\n",
        "- Can be easily used for multi-class classification. \n",
        "- It's closely related to linear classifiers we'll see in the next lecture. \n",
        "    - When we take the logarithms, the products (from the Naive bayes assumption) turn into summations. \n",
        "- Can provides a informative set of features from which to predict the class (next class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIvV3mNWYfPi"
      },
      "source": [
        "### Cons of naive Bayes\n",
        "\n",
        "- Assumes that spammers generate e-mails by picking words at random. It means that sentences have no syntax and content. Is that a fair assumption? \n",
        "    - oversimplification \n",
        "    - sometimes the best theories are the most oversimplified, provided their predictions are accurate, because they explain the most with the least. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7skehxxrYfPi"
      },
      "source": [
        "## Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrxHrL9fYfPi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.svm import SVC, SVR, LinearSVC\n",
        "\n",
        "# Other\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from plot_classifier import plot_classifier\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from plot_classifier import plot_classifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6rGIPkpYfPi"
      },
      "source": [
        "- Similarity-based algorithm.\n",
        "- Superficially, SVMs are more like weighted $k$-NNs. $k$-NNs are the Supervised Learning cousin of K-means.\n",
        "    - The decision boundary is defined by **a set of positive and negative examples** and **their weights** together with **their similarity measure**. \n",
        "    - A test example is a positive if on average it looks more like positive examples than the negative examples. \n",
        "\n",
        "- What makes SVMs special is that they only remember key examples (support vectors). So it's more efficient than other similarity-based algorithms.\n",
        "\n",
        "- SVMs use a similarity metric which is called a \"kernel\" in SVM land. A popular kernel is Radial Basis Functions (RBFs) which we will use today.\n",
        "- https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Q7XnC6eKYfPi",
        "outputId": "902e5408-e8e7-492d-b50c-f1586f1726df"
      },
      "source": [
        "cities_df = pd.read_csv('canada_usa_cities.csv')\n",
        "train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)\n",
        "canada = train_df.query('country == \"Canada\"')\n",
        "usa = train_df.query('country == \"USA\"')\n",
        "plt.scatter(canada[\"longitude\"], canada[\"latitude\"], color=\"red\", alpha=0.6)\n",
        "plt.scatter(usa[\"longitude\"], usa[\"latitude\"], color=\"blue\", alpha=0.6)\n",
        "plt.ylabel(\"latitude\")\n",
        "plt.xlabel(\"longitude\");"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lffe3kyXYfPi"
      },
      "source": [
        "# split into training/validation and testing set\n",
        "X_train, y_train = train_df.drop(columns=['country']), train_df['country']\n",
        "X_test, y_test = test_df.drop(columns=['country']), test_df['country']"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8bjGyiwYfPi",
        "outputId": "23a11dac-6a0b-444f-f9a2-b2cbf20b3e4c"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(gamma=0.01) # Ignore gamma for now\n",
        "svm.fit(X_train, y_train)\n",
        "print(\"Train scores: \", round(svm.score(X_train, y_train), 4))\n",
        "print(\"Test scores: \", round(svm.score(X_test, y_test), 4))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train scores:  0.8383\n",
            "Test scores:  0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "Fp8F7H_ZYfPj",
        "outputId": "7dd578c8-f50f-47c7-a1d7-7757eb7b76d2"
      },
      "source": [
        "# You can think of SVM with RBF kernel as \"smooth KNN\"\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"SVC\")\n",
        "plot_classifier(X_train, y_train, svm, ax=plt.gca());"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OjEmXi0YfPj"
      },
      "source": [
        "#### Support vectors \n",
        "\n",
        "- Each training example either is or isn't a \"support vector\".\n",
        "  - This gets decided during `fit`.\n",
        "\n",
        "- **Main insight: the decision boundary only depends on the support vectors.**\n",
        "\n",
        "- Let's look at the support vectors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRSluDWcYfPj"
      },
      "source": [
        "SVM is going to leverage the dataset and find the best values **$w^{(*)}$** and **$b^{(*)}$** for parameters $w$ and $b$. \n",
        "\n",
        "Then, the learning algorithm is defined as:\n",
        "\n",
        "> $f(x) = sign(w^{(*)}x − b^{(*)})$\n",
        "\n",
        "Now, to predict whether a **new** review is positive or negative using an SVM model, you have to take the text of the review, convert it into a feature vector, then multiply this vector by **$w^{(*)}$** , subtract **$b^{(*)}$** and take the sign of the result. \n",
        "\n",
        "If you get +1, then, the review is a positive review. If you get a -1, then, you have a negative review. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdxvlt6WYfPj"
      },
      "source": [
        "We want a hyperplane that separates positive examples from negative ones with the **largest margin**. \n",
        "\n",
        "If the margin or decision boundary is large, it contributes to a better generalization, that is how well the model will classify new examples in the future. \n",
        "\n",
        "<img src=\"img/svmplanes.PNG\" style=\"width: 700px;\"/>\n",
        "\n",
        "For two-dimensional feature vectors, we can easily visualize the solution. The blue and orange circles represent, respectively, positive and negative examples. The line given by $wx − b = 0$ is the decision boundary.\n",
        "\n",
        "That’s how Support Vector Machines work. This particular version of the algorithm builds the so-called linear model. It’s called linear because the decision boundary is a straight line (or a plane, or a hyperplane)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRfIFwQYfPj"
      },
      "source": [
        "<img src=\"img/img1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE59LOkYYfPj"
      },
      "source": [
        "# demo with a synthetic data set\n",
        "n = 20\n",
        "X = np.random.randn(n,2)\n",
        "y = np.random.choice((-1,+1),size=n)\n",
        "X[y>0,0] -= 2\n",
        "X[y>0,1] += 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8PTgDemYfPj"
      },
      "source": [
        "svm = SVC(kernel=\"linear\", C=1e6) # ignore the C=1e6 for now\n",
        "svm.fit(X,y)\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plot_classifier(X,y,svm, ax=plt.gca())\n",
        "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N8ybfTvYfPj"
      },
      "source": [
        "- The support vectors (SVs) are shown in yellow.\n",
        "- These are the example that \"support\" the boundary. \n",
        "\n",
        "Below: let's try removing all other examples, keeping only the SVs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKSVn0WYfPj"
      },
      "source": [
        "sv = svm.support_\n",
        "not_sv = list(set(range(n)) - set(sv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwGuD4R5YfPj"
      },
      "source": [
        "# remove all non-support vectors\n",
        "X3 = np.delete(X,not_sv,0)\n",
        "y3 = np.delete(y,not_sv,0)\n",
        "\n",
        "svm3 = SVC(kernel=\"linear\", C=1e6)\n",
        "svm3.fit(X3,y3)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plot_classifier(X,y,svm, ax=plt.gca())\n",
        "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
        "plt.title(\"Original\");\n",
        "plt.subplot(1,2,2)\n",
        "plot_classifier(X3,y3,svm3, ax=plt.gca(), lims=(X[:,0].min()-1,X[:,0].max()+1,X[:,1].min()-1,X[:,1].max()+1))\n",
        "plt.title(\"SVs only\");\n",
        "\n",
        "print(svm.coef_)\n",
        "print(svm3.coef_)\n",
        "print(\"The coefficients are the same!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amtrmjmVYfPk"
      },
      "source": [
        "# remove a support vector\n",
        "X2 = np.delete(X,sv[0],0)\n",
        "y2 = np.delete(y,sv[0],0)\n",
        "\n",
        "svm2 = SVC(kernel=\"linear\", C=1e6)\n",
        "svm2.fit(X2,y2);\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plot_classifier(X,y,svm, ax=plt.gca())\n",
        "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
        "plt.title(\"Original\");\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plot_classifier(X2,y2,svm2, ax=plt.gca())\n",
        "# plt.scatter(*svm2.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
        "plt.scatter(svm.support_vectors_[0,0], svm.support_vectors_[0,1], marker=\"x\", c=\"yellow\")\n",
        "plt.title(\"With one SV removed\");\n",
        "\n",
        "print(svm.coef_)\n",
        "print(svm2.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APArJwo5YfPk"
      },
      "source": [
        "- The **margin** is the distance from the boundary to the nearest point(s).\n",
        "- Maximizing the margin is a \"maximin\" problem: maximize the minimum distance to the boundary.\n",
        "- Intuitively, more margin is good because it leaves more \"room\" before we make an error.\n",
        "- Above: it looks like SVM maximizes the margin.\n",
        "- QUESTION: What does linear regression do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-CFwubYfPk"
      },
      "source": [
        "plt.figure()\n",
        "plot_classifier(X, y, svm, ax=plt.gca());\n",
        "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
        "plt.axis('equal');\n",
        "plt.axis('square');\n",
        "\n",
        "def SV_proj(svm):\n",
        "    v = svm.support_vectors_\n",
        "    s = np.array([svm.coef_.flatten()[1], -svm.coef_.flatten()[0]])\n",
        "    w = svm.coef_\n",
        "    return (v@s[:,None])/(s@s) * s - w/(w@w.T)*svm.intercept_\n",
        "proj = SV_proj(svm)\n",
        "\n",
        "for i in range(len(proj)):\n",
        "    p = proj[i]\n",
        "    sv = svm.support_vectors_[i]\n",
        "    plt.plot((p[0],sv[0]),(p[1],sv[1]), 'yellow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0x-k0LiYfPk"
      },
      "source": [
        "### Hyperparameters of SVM \n",
        "\n",
        "- Key hyperparameters of `rbf` SVM are\n",
        "    - `gamma`\n",
        "    - `C`\n",
        "    \n",
        "- We are not equipped to understand the meaning of these parameters at this point but you are expected to describe their relation to the fundamental tradeoff. \n",
        "\n",
        "See [`scikit-learn`'s explanation of RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrzVS5QsYfPk"
      },
      "source": [
        "### Relation of `gamma` and the fundamental trade-off\n",
        "\n",
        "- `gamma` controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
        "  - larger `gamma` $\\rightarrow$ more complex $\\rightarrow$ can lead to overfitting\n",
        "  - smaller `gamma` $\\rightarrow$ less complex $\\rightarrow$ can lead to underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68q8gaGYfPk"
      },
      "source": [
        "### Relation of `C` and the fundamental trade-off\n",
        "\n",
        "- `C` _also_ affects the fundamental tradeoff\n",
        "    - larger `C` $\\rightarrow$ more complex $\\rightarrow$ can lead to overfitting\n",
        "    - smaller `C` $\\rightarrow$ less complex $\\rightarrow$ can lead to underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKjBqT6QYfPk"
      },
      "source": [
        "**C helps us deal with Noise**\n",
        "\n",
        "To extend SVM to cases in which the data is not linearly separable, we introduce the Hinge Loss Function: \n",
        "\n",
        "$max(0, 1− y_i(wx_i − b))$. *don't worry too much about this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z33s4u7zYfPk"
      },
      "source": [
        "The Hinge Loss Function is zero if $wx_i$ lies on the correct side of the decision boundary. \n",
        "For data on the wrong side of the decision boundary, the function’s value is proportional to the distance from the decision boundary.\n",
        "\n",
        "\n",
        "By changing the Hyperparamenter C, we determine the tradeoff between increasing the size of the decision boundary and trying to get each $x_i$ in the correct side of the decision boundary. \n",
        "\n",
        "SVMs that optimize hinge loss are called **soft-margin SVMs**, while the original formulation is referred to as a **hard-margin SVM**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPD6LXZ4YfPk"
      },
      "source": [
        "For sufficiently high values of C, the second term in the cost function will become negligible, so the SVM algorithm will try to find the highest margin by completely ignoring misclassification. As we decrease the value of C, making classification errors is becoming more costly, so the SVM algorithm tries to make fewer mistakes by sacrificing the margin size. **A larger margin is better for generalization.**\n",
        "\n",
        "C regulates the tradeoff between classifying the training data well (minimizing empirical risk) and classifying future examples well (generalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJ9GAHyYfPk"
      },
      "source": [
        "<img src=\"img/img4.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obdCdDyzYfPl"
      },
      "source": [
        "### The Kernel Trick\n",
        "\n",
        "1. What if there’s noise in the data and no hyperplane can perfectly separate positive examples from negative ones?\n",
        "2. What if the data cannot be separated using a plane, but could be separated by a higher-order polynomial?\n",
        "\n",
        "SVM can also incorporate kernels that can make the decision boundary arbitrarily non-linear. In some cases, it could be impossible to perfectly separate the two groups of points because of noise in the data, errors of labeling, or outliers. \n",
        "\n",
        "You can see both situations in the next figure. In the left case, the data could be separated by a straight line if not for the noise (outliers or examples with wrong labels). \n",
        "\n",
        "In the right case, the decision boundary is a circle and not a straight line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPryDZN6YfPl"
      },
      "source": [
        "<img src=\"img/img2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlNA9PNKYfPl"
      },
      "source": [
        "**Dealing with Inherent Non-Linearity**\n",
        "SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. Indeed, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. In SVMs, using a function to implicitly transform the original space into a higher dimensional space during the cost function optimization is called the kernel trick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mdXFJ_MYfPl"
      },
      "source": [
        "<img src=\"img/img3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFub6q-7YfPl"
      },
      "source": [
        "It’s possible to transform a two-dimensional non-linearly-separable data into a linearly-separable threedimensional\n",
        "data using a specific mapping. For example: 2D data into a 3D space. Now, the data becomes\n",
        "linearly separable in the transformed space.\n",
        "\n",
        "However, we don’t know a priori which mapping would work for our data. \n",
        "\n",
        "\n",
        "\n",
        "By using the kernel trick, we can get rid of a costly transformation of original feature vectors into higher dimensional vectors and avoid computing their dot-product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVKop-upYfPl"
      },
      "source": [
        "n = 20\n",
        "d = 2\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(n,d)\n",
        "y = np.sum(X**2,axis=1) < 0.4\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y);\n",
        "plt.xlabel(\"$x_{i1}$\", fontsize=20);\n",
        "plt.ylabel(\"$x_{i2}$\", fontsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM5vsKpYfPl"
      },
      "source": [
        "svm = SVC(kernel=\"linear\")\n",
        "svm.fit(X,y)\n",
        "\n",
        "plot_classifier(X,y,svm)\n",
        "\n",
        "print(\"Training accuracy\", svm.score(X,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGlpOXyKYfPl"
      },
      "source": [
        "Z = X**2\n",
        "\n",
        "plt.scatter(Z[:,0], Z[:,1], c=y);\n",
        "plt.xlabel(\"$z_{i1}$\", fontsize=20);\n",
        "plt.ylabel(\"$z_{i2}$\", fontsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB2b3XH0YfPl"
      },
      "source": [
        "svm = SVC(kernel=\"linear\", C=100)\n",
        "svm.fit(Z,y)\n",
        "\n",
        "plot_classifier(Z,y,svm)\n",
        "\n",
        "print(\"Training accuracy\", svm.score(Z,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biUT33jNYfPm"
      },
      "source": [
        "plot_classifier(X, y, svm, transformation=lambda X: X**2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cTpU1ujYfPm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}